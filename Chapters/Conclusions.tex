\chapter{Conclusions} \label{cha:conclusions}

Today, inference and statistics are crucial to the scientific process. Over a century ago, Ernest Rutherford is reported to have said: ``If you need statistics, you did the wrong experiment" \cite{Blackett}. While this sentiment may have been somewhat valid in the past, it no longer aligns with the current state of the scientific process. Modern research often involves complex models designed to explain subtle phenomena within multidimensional datasets. Additionally, the volume of data available to astrophysicists and cosmologists has expanded dramatically over the past two decades and continues to increase. Consequently, our statistical analysis tools must evolve in sophistication to match this growth. Now, the primary constraint on our understanding of the universe is not the volume of data we possess, but our capacity to analyze, interpret, and derive meaningful insights from it \cite{Trotta:2017wnx}.

Simulation-based inference techniques offer a qualitative shift in our approach to statistical inference and a promising avenue to fully exploit the data potential. The present work highlights several key benefits of this framework, and argues for its central role in the modern physics data analysis toolkit. 
The significance of the three applications presented in this thesis is underscored not only by the complexity of the statistical inference challenges they tackle, but also by the critical scientific questions they seek to resolve. 
Specifically, simulation-based inference can solve \emph{hierarchical inference} of dark matter substructure population parameters from a dataset of strong lensing images, constraining dark matter nature, while fully accounting from the uncertainties coming from all the building blocks of the model (\ie, lens, source, and substructure population). It also can solve high-dimensional inference at the \emph{field-level}, providing a means to optimally extract information from upcoming cosmological surveys. Lastly, simulation-based inference allows, for the first time to the authors' knowledge, for \emph{self-consistent} measurement of population parameters based on detected and undetected objects in sky-maps, by making observational biases related to the point source detection part of the model itself. These applications demonstrate the versatility of simulation-based inference in tackling diverse and intricate problems, setting the stage for its broader adoption and further innovation in future research.

We are already witnessing a widespread adoption and advancement of simulation-based inference techniques for cosmological and astrophysical data analysis. However, these techniques have yet to realize their full potential, and numerous challenges remain open. Key questions include: Can the selection of optimal neural networks architectures be automatized? How to handle situations with high volume data? What is the most efficient way to sample from constrained likelihood regions in very high dimensions for sequential inference? How to reliably perform goodness-of-fit tests and identify model misspecification? Crucially, the further development of these techniques must be accompanied by the development of robust performance tests and diagnostics. All these open threads, and possibly more, are active areas of research within the community.

Our pursuit to develop powerful, fast, and robust analysis techniques is especially urgent because of the ever-increasing influx and quality of data from current and forthcoming observatories.
The potential for using these observations to uncover new physics in the dark sector are therefore very bright. Now is absolutely an opportune time to explore innovative ways of maximizing the discovery potentials of these datasets. While the approaches presented in this thesis are far from the only interesting avenues for analyzing this wealth of data for new physics searches, they represent concrete steps towards fully exploiting these measurements to refine our understanding of physical laws.

%Indeed, \gls*{sbi} provides approximate solutions for the comparison of complex models with high-quality data, making it a powerful tool when exact methods fall short. While exact methods might offer precise results, they often do so at the cost of oversimplifying the model, which is particularly risky when dealing with detailed data. By embracing the approximate nature of \gls*{sbi}, we can tackle the right questions with the complexity they deserve, yielding more meaningful insights with accurately quantified uncertainty.




