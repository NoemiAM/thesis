\chapter{Conclusions and Outlook} \label{cha:conclusions}

The primary goal of this thesis is to enable new physics searches by addressing the statistical and computational challenges that arise in the fields of astrophysics and cosmology. 
The analysis of complex data described by intricate models is challenging due to computational limitations. Traditional methods in modern astrophysical and cosmological data analysis are sampling-based inference techniques like Markov-chain Monte Carlo and nested sampling methods. However, these approaches frequently rely on approximate likelihoods and suffer from a significant drawback: the time required to achieve convergence scales poorly with the dimensionality of the explored parameter space. This thesis contributes to develop and establish an alternative approach based on novel \gls*{sbi} techniques, that have seen a remarkable development in recent years. 
%These techniques offer a qualitative shift in our approach to statistical inference and a promising avenue to fully exploit the data potential. 

We began in Chapter~\ref{cha:sbi} by describing how \gls*{sbi} approaches achieve robust Bayesian inference given an implicit representation of the likelihood through a generative model. Thanks to their unique properties, one can coherently segment the analysis into manageable tasks (\eg~parameter inference, object detection, image reconstruction) that target distinct lower-dimensional parts, or “blocks”, of a large model. The inference carried out on each block will take into account the uncertainties coming from the other blocks, thus correctly propagating uncertainties between model components. This approach flexibly modularizes the analysis of large models and then coherently synthesizes it for a comprehensive understanding. It enables one to draw conclusions based on the full model, ultimately making the analysis statistically sound, simulation efficient, and versatile. 

Initially, we focused on the analysis of strong lensing images as a dark matter probe (Chapter~\ref{cha:lensing}). This is a primary example of applicability of \gls*{sbi} to a large forward model. Our results demonstrated that a sequential \gls*{sbi} technique called \gls*{tmnre} enables precise marginal and targeted inference, overcoming traditional computational challenges. We further showed the application of hierarchical inference to extract the dark matter cutoff mass signal from a dataset of lenses.

We then tackled the inverse problem that goes from non-linear, non-local mappings of late-time density fields to Gaussian cosmological initial conditions. To this end, we employed autoregressive Gaussian likelihood estimation to model the conditional dependencies between pixels in the density field. Posterior sampling is achieved through a Gibbs sampling algorithm based on exact data augmentation, ensuring efficient exploration of high-dimensional parameter spaces. The proposed approach combines computational efficiency with applicability to generic, non-differentiable forward simulators, making it suitable for broader astrophysical and cosmological data analysis tasks.

Finally, we turned towards the problem of point source detection and population parameter inference in sky-maps. In Chapter~\ref{cha:detection}, we developed a highly interpretable (since it resembles components of traditional survey analysis workflows) \gls*{sbi} framework that allows, for the first time to the authors' knowledge, to perform consistently point source detection and population parameters inference, from both detected and sub-threshold. This was possible by defining source detection as a novel and high-dimensional form of prior truncation to incorporate detected sources into the simulation model.

Overall, this thesis aimed to highlight the potential of \gls*{sbi} for astrophysical data analysis, positioning this framework as an essential part of the modern physics data analysis toolkit.


\subsection*{Outlook}

We are already witnessing a widespread adoption and advancement of simulation-based inference techniques for cosmological and astrophysical data analysis. However, these techniques have yet to realize their full potential, and numerous challenges remain open. Key questions include: Can the selection of optimal neural networks architectures be automatized? How to handle situations with high volume data? What is the most efficient way to sample from constrained likelihood regions in very high dimensions for sequential inference? How to reliably perform goodness-of-fit tests and identify model misspecification? Crucially, the further development of these techniques must be accompanied by the development of robust performance tests and diagnostics. All these open threads, and possibly more, are active areas of research within the community.

Our pursuit to develop powerful, fast, and robust analysis techniques is especially urgent because of the ever-increasing influx and quality of data from current and forthcoming observatories.
The potential for using these observations to uncover new physics in the dark sector are therefore very bright. Now is absolutely an opportune time to explore innovative ways of maximizing the discovery potentials of these datasets. While the approaches presented in this thesis are far from the only interesting avenues for analyzing this wealth of data for new physics searches, they represent concrete steps towards fully exploiting these measurements to refine our understanding of physical laws.
%to search for physics beyond the Standard Model and novel astrophysical phenomena in the scientific feedback-loop.

\todo{Not sure about including this or stop at the paragraph above:}
Indeed, \gls*{sbi} provides approximate solutions for the comparison of complex models with high-quality data, making it a powerful tool when exact methods fall short. While exact methods might offer precise results, they often do so at the cost of oversimplifying the model, which is particularly risky when dealing with detailed data. By embracing the approximate nature of \gls*{sbi}, we can tackle the right questions with the complexity they deserve, yielding more meaningful insights with accurately quantified uncertainty.

%“[...] as emphasized in Rubin (1984), one of the great scientific advantages of simulation analysis of Bayesian methods is the freedom it gives the researcher to formulate appropriate models rather than be overly interested in analytically neat but scientifically inappropriate models.” (Gelman, 1996)
%
%“Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise. ” (John Tuckey, 1962)




