\chapter{Introduction} \label{sec:introduction}

The fundamental goal of the physical sciences is to understand the laws that govern physical phenomena.
This process inherently involves a continuous interplay between theoretical models and physical observations, forming a feedback loop that continuously refines both. By comparing observational data with model predictions, model validity can be tested, areas of improvements can be identified, and their faithfulness to reality can be improved. Conversely, theoretical models can guide observational strategies by predicting phenomena that have yet to be observed, hence focusing the attention to specific phenomena or conditions that may yield new insights.

A key ingredient of this scientific process is \emph{statistics}. Statistics is a rigorous mathematical language that enables formal statements about what event is possible under physical laws, thus bridging the gap between physics models and observational data.
Guided by this formalization, given some parameters that describe a physical system, implied \emph{predictions} or consequences of a physical model can be computed, allowing for the systematical exploration of the model's implications. Predictions are almost never exact and are intrinsically statistical, \eg\ due to the randomness of the phΩysical processes, the measurement processes, or incomplete information.
%This approach is known as forward modeling and the above-mentioned physical parameters are called the model parameters.
In order to refine theoretical models, it is essential to perform the inverse process: starting with the effects to discover the causes, inferring from a set of observations the causal factors that produced them. This task, known as solving an \emph{inverse problem}, involves mapping back observational data to infer the underlying model parameters that are not directly observable.

In astrophysics and cosmology, this iterative cycle between prediction and inference is particularly challenging due to the complex nature of the systems under study. 
Additionally, we are at the dawn of a data-driven era in astrophysics and cosmology. The coming decade will see transformative science conducted by recent and upcoming observatories, based both on the ground (\eg\ Rubin-LSST \cite{LSSTDarkEnergyScience:2012kar}, ELT \cite{Simon:2019aa}), and space-based missions (\eg\ JWST \citep{Gardner:2006ky}, Euclid \cite{Refregier:2010ss}). Astrophysical data will increase not only in quantity, but also in complexity and detail, promising unprecedented high-precision measurements of the growth of structure and geometry of the universe. This will open new windows to dark matter, dark energy, neutrino physics, inflationary cosmology, and gravity tests. 
Given the unprecedented size and detail of these data, connecting theoretical models with this wealth of high-precision observations presents significant challenges. First, information must be optimally extracted from the data to avoid discarding valuable insights. Second, uncertainties must be correctly treated and thoroughly propagated to ensure accurate scientific statements. Hence, the need for principled statistical analyses has never been more critical. Maximizing information extraction from future experiments, and making robust scientific statements with well quantified uncertainties, are crucial for our ability to uncover new physics. Fully exploiting this data for scientific purposes will require increasingly detailed and complex physical models, which bring along higher computational costs, as well as a larger number of uncertain parameters, including those characterizing signal and background systematics. Developing corresponding theoretical and computational tools to address these challenges for the statistical analysis of astrophysical data is the focus of this thesis.

In recent years, remarkable progress in computing technologies and programming languages have made it possible to express these increasingly detailed and complex physical models through computer \emph{simulators}. Physics simulators serve as powerful predictive devices, mapping model parameters into realized data by reproducing numerically the underlying natural phenomenon of interest. However, while simulators excel at predicting system behaviors, they are poorly suited for statistical inference and for solving inverse problems. 
Broadly speaking, to evaluate the likelihood of a data realization implicitly defined through a computer simulator one must solve an inverse problem that involves integrating all possible code paths, for all possible simulator configurations, that could have potentially led to the observed data realization. Clearly, as the fidelity and detail of modern computer simulations increase, computing this quantity becomes exceedingly difficult, if not entirely intractable or computationally infeasible. 
%Broadly speaking, identifying data-consistent model configurations requires numerical evaluation of the so-called likelihood function—which captures the relative consistency of any given model configuration with observations.
%Unfortunately, conventional statistical inference often poses prohibitively strict restrictions on which models can be used. 
%This creates a dilemma: When we design models with statistical considerations in mind, we have a close feedback loop between data and theory. However, this poses tight constraints on what models can be used. This strategy often only yields limited insights about underlying causal mechanisms. In contrast, when we design interpretable mechanistic models predestined for this purpose, for example, relying on high-fidelity computer simulations, we lose the ability to perform (likelihood-based) statistical inference. Constraining these models by observed data often poses an extremely difficult challenge.

Fortunately, we live in an age of extreme technological innovation and unprecedented computational capabilities. In particular, recent advances in  deep learning and differentiable programming have led to the emergence and proliferation of a \emph{simulation-based inference paradigm} that can effectively tackle the above challenges. By leveraging the power of neural networks, these new methods can approximate the complex relationships within simulators, allowing for efficient solutions to inverse problems that were previously beyond reach.

Motivated by this paradigm shift in statistical analysis, this thesis intends to illustrate possible paths towards further refinement of the scientific loop for astrophysical data in a simulation-based setting. Furthermore, it aims to highlight the potential of neural simulation-based inference in improving the quality of insight we can gain from simulations, maximizing information extraction from data, and providing robust uncertainty quantification for scientific statements.


%Example
%To give the reader some additional intuition as to why the likelihood is intractable, let us briefly consider a metaphor that has been popularized by Kyle Cranmer and Gilles Louppe amongst others. It starts from the premises that the popular Galton board, as depicted in
%Figure 1.3, could be viewed as a scientific simulator. The possible set of simulator outputs correspond to the various bins of the Galton board into which the beads can end up, whereas the simulator’s configu- ration or free parameters relate to the position or bias of the various pegs. To simplify the discussion, let us make the assumption that there are n + 1 bins for n rows of pegs. Contrary to most simulators, the likelihood of a bead ending up in a particular bin does have a tractable likelihood whenever we consider an idealized Galton board. Under this assumed model, the probability of a bead ending up in bin k when counting from the left is defined as
%nkpk(1 − p)n−k, (1.1)
%where p is the probability of a bead bouncing to the right. Recall that the evaluation of the likelihood depends on the integration of all possible code paths that could have produced the observed data. If we view the bead traveling through the Galton board as an execution trace of a computer program with stochastic function calls, then the number of possible paths the computer code can take to produce a bead in bin k is fully described by the binomial coefficient (nk). However, evaluating this likelihood analytically would not be possible if we were to change the position or bias of various pegs. In that case we would not be able to analytically describe the likelihood in the same way, but we would still be able to sample from the simulation model by simply dropping beads into the Galton board!
%
%While the Galton board metaphor demonstrates that even for conceptual problems the computation of the likelihood quickly becomes impractical, the metaphor does not imply that statistical inference in these settings is impossible. In fact, one can still rely on approximate inference as long as it is likelihood-free. This is easier said than done as virtually all statistical inference relies on the likelihood in some way. However, the idea is that surrogates can be constructed that do not rely on the direct evaluation of the likelihood but rather produce estimates of key quantities necessary for statistical inference, be it numerically or otherwise. For instance, one such intractable quantity that is central to this dissertation is the Bayesian posterior
%p(\theta | x) ≜ p(\theta) p(x | \theta), (1.2) p(x)
%where the marginal model
%p(x)≜Z d\theta p(\theta)p(x|\theta), (1.3)
%for a given prior p(\theta) quantifying the initial belief about the free parameters \theta.
%


\subsection*{Outline}
This thesis aims to contribute to the ongoing effort to transition towards simulation-based inference techniques in astrophysics and cosmology, emphasizing some of the tremendous opportunities that this transition brings. To this end, this thesis first proposes a general simulation-based ecosystem for astrophysical data analysis (Chapter~\ref{cha:sbi}). Then, it illustrate its capabilities through three exemplary applications to astrophysics: 
\begin{itemize}[leftmargin=1cm]
	\item the analysis of strong gravitational lenses as a dark matter probe in Chapters~\ref{cha:lensing} and \ref{cha:anre}, 
	\item the reconstruction of cosmological initial conditions from late-time density fields in Chapter~\ref{cha:cosmo}, 
	\item and the analysis of point-sources in sky-maps in Chapter~\ref{cha:detection}. 
\end{itemize}
Overall, it aims to highlight the potential for fast, flexible, and testable simulation-based algorithms to facilitate scientific discovery in astrophysics and cosmology, at the dawn of their data-driven era, and forward.




